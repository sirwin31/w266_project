{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaulate Roberta Model on Politifact Test Data\n",
    "### 1 Dec 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Evaluation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Evaulation Parameters\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "MODEL_FILE =\"saved_model_epoch10_20201129_0424.tar\"\n",
    "TEST_DATASET = \"test_data_1Dec_roberta_pf.pickle\"\n",
    "EVAL_BATCH_SIZE = 16\n",
    "DESCRIPTION = \"PolitiFact only Roberta model on test data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Setup and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn.metrics\n",
    "import tqdm\n",
    "import torch\n",
    "import transformers as hft\n",
    "\n",
    "sys.path.insert(0, \"/home/jupyter\")\n",
    "import util.log\n",
    "import util.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation data\n",
    "with open(TEST_DATASET, \"rb\") as vfile:\n",
    "    test_dataset = pickle.load(vfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load saved model\n",
    "checkpoint = torch.load(MODEL_FILE)\n",
    "model = (hft.RobertaForSequenceClassification\n",
    "         .from_pretrained(MODEL_NAME))\n",
    "model.load_state_dict(checkpoint[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to GPU if available\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "model.to(device)\n",
    "model.eval();\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Evaluation Loop and Results Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:05,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Evaulate model\n",
    "eval_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                batch_size=EVAL_BATCH_SIZE,\n",
    "                                shuffle=False)\n",
    "labels = []\n",
    "preds = []\n",
    "logits = []\n",
    "probs = []\n",
    "sources = []\n",
    "article_tokens = []\n",
    "file_names = []\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for art_num, article in tqdm.tqdm(enumerate(eval_loader)):\n",
    "        input_ids = article[\"input_ids\"].to(device)\n",
    "        attent_mask = article[\"attention_mask\"].to(device)\n",
    "        output = model(input_ids,\n",
    "                       attention_mask=attent_mask,\n",
    "                       output_hidden_states=False,\n",
    "                       output_attentions=False)\n",
    "        prob = softmax(output[0].detach().cpu()).numpy()\n",
    "        logit = output[0].detach().cpu().numpy()\n",
    "        pred = [0 if lgt[0] > lgt[1] else 1 for lgt in logit]\n",
    "        label = article[\"labels\"].numpy()\n",
    "        labels.extend(label)\n",
    "        preds.extend(pred)\n",
    "        logits.extend(logit)\n",
    "        probs.extend(prob)\n",
    "        article_tokens.extend(article[\"article_tokens\"].numpy())\n",
    "        sources.extend(article[\"sources\"])\n",
    "        file_names.extend(article[\"file_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Source</th>\n",
       "      <th>Token_Length</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Logit_real(0)</th>\n",
       "      <th>Logit_fake(1)</th>\n",
       "      <th>Prob_real(0)</th>\n",
       "      <th>Prob_real(1)</th>\n",
       "      <th>File_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>politifact</td>\n",
       "      <td>1163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.827148</td>\n",
       "      <td>-3.899718</td>\n",
       "      <td>0.999559</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>politifact35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>politifact</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.737876</td>\n",
       "      <td>3.626107</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.999367</td>\n",
       "      <td>politifact4028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>politifact</td>\n",
       "      <td>505</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.127038</td>\n",
       "      <td>-4.195304</td>\n",
       "      <td>0.999757</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>politifact567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>politifact</td>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.686815</td>\n",
       "      <td>4.649739</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.999912</td>\n",
       "      <td>politifact14342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>politifact</td>\n",
       "      <td>506</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.678712</td>\n",
       "      <td>4.526700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>politifact15427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>119</td>\n",
       "      <td>politifact</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.874329</td>\n",
       "      <td>3.699367</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.999486</td>\n",
       "      <td>politifact15108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>120</td>\n",
       "      <td>politifact</td>\n",
       "      <td>11333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.112895</td>\n",
       "      <td>-4.151903</td>\n",
       "      <td>0.999743</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>politifact2881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>121</td>\n",
       "      <td>politifact</td>\n",
       "      <td>19813</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.220101</td>\n",
       "      <td>-4.236417</td>\n",
       "      <td>0.999788</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>politifact809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>122</td>\n",
       "      <td>politifact</td>\n",
       "      <td>587</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.772211</td>\n",
       "      <td>-4.076408</td>\n",
       "      <td>0.999610</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>politifact13305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>123</td>\n",
       "      <td>politifact</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.738001</td>\n",
       "      <td>4.677998</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.999919</td>\n",
       "      <td>politifact13827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Article      Source  Token_Length  Predictions  Labels  Logit_real(0)  \\\n",
       "0          0  politifact          1163            0       0       3.827148   \n",
       "1          1  politifact          1442            1       0      -3.737876   \n",
       "2          2  politifact           505            0       0       4.127038   \n",
       "3          3  politifact           260            1       1      -4.686815   \n",
       "4          4  politifact           506            1       1      -4.678712   \n",
       "..       ...         ...           ...          ...     ...            ...   \n",
       "119      119  politifact           145            1       1      -3.874329   \n",
       "120      120  politifact         11333            0       0       4.112895   \n",
       "121      121  politifact         19813            0       0       4.220101   \n",
       "122      122  politifact           587            0       0       3.772211   \n",
       "123      123  politifact           163            1       1      -4.738001   \n",
       "\n",
       "     Logit_fake(1)  Prob_real(0)  Prob_real(1)        File_Name  \n",
       "0        -3.899718      0.999559      0.000441     politifact35  \n",
       "1         3.626107      0.000633      0.999367   politifact4028  \n",
       "2        -4.195304      0.999757      0.000243    politifact567  \n",
       "3         4.649739      0.000088      0.999912  politifact14342  \n",
       "4         4.526700      0.000100      0.999900  politifact15427  \n",
       "..             ...           ...           ...              ...  \n",
       "119       3.699367      0.000514      0.999486  politifact15108  \n",
       "120      -4.151903      0.999743      0.000257   politifact2881  \n",
       "121      -4.236417      0.999788      0.000212    politifact809  \n",
       "122      -4.076408      0.999610      0.000390  politifact13305  \n",
       "123       4.677998      0.000081      0.999919  politifact13827  \n",
       "\n",
       "[124 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label of 0 is real, 1 is fake\n",
    "eval_results = pd.DataFrame({\"Article\": list(range(len(labels))),\n",
    "                             \"Source\": sources,\n",
    "                             \"Token_Length\": article_tokens,\n",
    "                             \"Predictions\": preds,\n",
    "                             \"Labels\": labels,\n",
    "                             \"Logit_real(0)\": [x[0] for x in logits],\n",
    "                             \"Logit_fake(1)\": [x[1] for x in logits],\n",
    "                             \"Prob_real(0)\": [x[0] for x in probs],\n",
    "                             \"Prob_real(1)\": [x[1] for x in probs],\n",
    "                             \"File_Name\": file_names})\n",
    "\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data to csv file\n",
    "eval_results.to_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Overall Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'PolitiFact only Roberta model on test data',\n",
       " 'eval_notes': 'Evaluating PolitiFact data',\n",
       " 'model': 'saved_model_epoch10_20201129_0424.tar',\n",
       " 'epochs': 10,\n",
       " 'batch_size': 16,\n",
       " 'Date_evaluated': '1Dec2020',\n",
       " 'train_data': 'train_data_28Nov_roberta_pf.pickle',\n",
       " 'test_data': 'test_data_1Dec_roberta_pf.pickle',\n",
       " 'accuracy': 0.9435483870967742,\n",
       " 'precision': 0.8870967741935484,\n",
       " 'recall': 1.0,\n",
       " 'f1': 0.9401709401709402}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overall metrics\n",
    "precision, recall, f1, _ = sklearn.metrics.precision_recall_fscore_support(\n",
    "    eval_results.Labels, eval_results.Predictions, average=\"binary\")\n",
    "accuracy = sklearn.metrics.accuracy_score(eval_results.Labels, eval_results.Predictions)\n",
    "\n",
    "metrics = {\"description\": DESCRIPTION,\n",
    "           \"eval_notes\": \"Evaluating PolitiFact data\",\n",
    "           \"model\": MODEL_FILE,\n",
    "           \"epochs\": checkpoint[\"epoch\"],\n",
    "           \"batch_size\": EVAL_BATCH_SIZE,\n",
    "           \"Date_evaluated\": \"1Dec2020\",\n",
    "           \"train_data\": \"train_data_28Nov_roberta_pf.pickle\",\n",
    "           \"test_data\": TEST_DATASET,\n",
    "           \"accuracy\": accuracy,\n",
    "           \"precision\": precision,\n",
    "           \"recall\": recall,\n",
    "           \"f1\": f1}\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to JSON text file.\n",
    "with open(\"test_metrics.json\", \"wt\") as jfile:\n",
    "    json.dump(metrics, jfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gossip</th>\n",
       "      <th>political</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gossip</th>\n",
       "      <td>62</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>political</th>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           gossip  political\n",
       "gossip         62          7\n",
       "political       0         55"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = pd.DataFrame(sklearn.metrics.confusion_matrix(eval_results.Labels, eval_results.Predictions))\n",
    "label_titles = {0: \"gossip\", 1: \"political\"}\n",
    "cm.rename(index=label_titles, columns=label_titles, inplace=True)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
